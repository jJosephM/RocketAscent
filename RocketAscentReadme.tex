\documentclass[11pt]{article}

\linespread{1.3}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\numberwithin{equation}{section}
%\usepackage{xcolor}
\usepackage[table]{xcolor}
\usepackage{amsfonts}
\usepackage{multirow}

\begin{document}
\title{Rocket Ascent Guidance Readme}
\author{Jacob Mankenberg}
\maketitle



This project is a little outside my area of ``expertise" if you can say I have any, though I'm surprised how well I've understood what I've read so far and just how much my training in physics has prepared me for these topics. For one, the basic premise of the mathematical theory the project employs is variational calculus, something I'm quite familiar with - at least in a few applications to classical mechanics, quantum field theory, thermodynamics and spin lattice models, general relativity and field theory, and some statistical mechanics. In addition, some of the mathematical analysis on which variational principles are built is a common starting point for the study of differentiability on manifolds. In fact, the very first thing discussed in the well known text by Nicolaescu \ref{Nicolaescu} is the notion of a differentiable mapping between Banach spaces. This in turn defines the \textit{Frechet} derivative which a physicist might recognize as the Euler - Lagrange equations. Again, something I am familiar with. Who knew?

This project however, takes a basic variational principle a little further. Graduate texts in mechanics such as Fetter and Walecka \ref{FetWal} and Goldstein \ref{Goldstein} discuss constraints on variational problems, even naming them for easy reference in a github readme on rocket ascent guidance, but do not go much further in developing the general mathematical principles on constrained optimization problems. I needed to learn a bit about optimal control. I will be solely referencing Kamien and Schwartz \ref{OC} in this project - it is considered a standard in the field. I found this book interesting, useful, thought provoking and learned some things that I found a bit surprising. It is meant for analytic economics but is general enough to be useful anywhere, including here.

\section{Mathematical Background on Optimal Control Theory}
I'll populate this section with a description of the elements of optimal control needed for the program. We'll discuss the specific application and implementation later when covering the actual ascent guidance principle.

\paragraph{The Variational Principle}
Variational calculus as I know it is basically the extension of finding local extrema in calculus to functionals: functions that map from some function space to the real numbers. In calculus, you compute the derivative of a function and find the zeroes of the derivative to give the points that are either maxima or minima of the functions. We can think about this as varying the independent variable and measuring the change in the function until we find a point where a first order variation does not yield a change in the function. In an analogous manner, for a functional, we vary the independent function - akin to the variable - and find that function whose first order variation does not yield a change in the output of the functional. In other words, we find the function that minimizes or maximizes the value of the functional by changing it until the functional derivative is zero.

In practice it's a little more involved than a simple derivative. We are varying a function...any function. So we need a systematic way to deal with how and where it's being changed and what it can be changed into to be a valid solution. The function may need to satisfy some boundary conditions or some sort of constraint - in the form of an equality, inequaltiy or differential equation - and we need to be able to accomodate those conditions. For this project that's exactly the problem we're trying to solve. We need optimal control.

\subsection{Optimal Control Theory}
Let's get right into it. There are 2 types of variables in this problem: a control function $u(t)$ and a state variable $x(t)$ governed by a first order differential equation. The simplest problem is to select a control function $u(T)$ to maximize
\begin{equation}
\int_{t_0}^{t_1}f(x(t),u(t),t)dt
\end{equation}
subject to the following
\begin{align}
x'(t) = g(t,x(t),u(t)) \\
x(t_0) = x_0, \text{fixed} \\
x(t_1) = \text{free}
\end{align}
The integral is directly influence by $u(t)$ and $x'(t)$ is indirectly influence by $u(t)$ through the differential equation, so even if the goal is to know the form of $x(t)$, knowing $u$ is sufficient.

A standard calculus of variations problem is to maximize
\begin{equation}
\int_{t_0}^{t_1}f(x(t),x'(t),t)dt
\end{equation}
subject to
\begin{equation}
x(t_0) = x_0
\end{equation}
which in optimal control is 
\begin{equation}
\int_{t_0}^{t_1}f(x(t),u(t),t)dt
\end{equation}
subject to
\begin{align}
x'(t) = u(t) \\
x(t_0) = x_0, \text{fixed}
\end{align}
In general, there are an arbitrary number of control variables and higher order derivaties which may be reduced to a first order problem (though we won't need that here). Thus we see that variational calculus - which may include any number of derivatives of $x(t)$ in the integrand - is a relaxed subset of the problems addressed by optimal control. In fact, I would guess that that is how the field came about.

\subparagraph{Simplest Optimal control Problem}
As with any theory, it's often easiest to start with a simplified version of the problem you're trying to solve. In classical mechanics this simplification could be considering point particles and frictionless surfaces, in thermodynamics it's adiabatic systems, in quantum mechanics...a single spinless particle in a box. Here we start with a free variable at the terminal point and invoke a Lagrange multiplier procedure to find the maximizing solution $u(t) = u^*(t)$ and $x(t) = x^*(t)$:
\begin{align}
\int_{t_0}^{t_1}f(x(t),u(t),t)dt = \int_{t_0}^{t_1}f(x(t),u(t),t)dt + \int_{t_0}^{t_1}dt\lambda(t)(f(x(t),u(t),t) - x'(t)) \\
\text{then by parts } - \int_{t_0}^{t_1}dt\lambda(t)x'(t) = -\lambda(t)x(t)|_{t_0}^{t_1} + \int_{t_0}^{t_1}dt\lambda'(t)x(t)
\end{align}
so that 
\begin{equation}
\int_{t_0}^{t_1}f(x(t),u(t),t)dt = \int_{t_0}^{t_1}dt\big[f(x(t),u(t),t) + \lambda(t)g(x(t),u(t),t)+\lambda'(t)x(t)\big] - \lambda(t)x(t)|_{t_0}^{t_1}
\end{equation}
The control function $u(t)$ with $x(t_0) = x_0$ and $x'(t) = g(t,x(t),u(t))$ determine the path $x^*(t)$ so the variation will come through $u \to u^* + ah(t)$. Here $h$ is the variation with $a$ being the variable parameter - remember there's multiple ways to explicitly do this. Here we're not using a functional derivative. If we let $y(t,a)$ be the state variable described/generated by $u$ so that $y(t,a=0) = x^*$ is the optimal state function and $y(t_0,a) = x^*_0$, the \textit{functional} dependent on $a$ is
\begin{equation}
J(a) = \int_{t_0}^{t_1}f(y(t,a),u*(t)+ah(t),t)dt = \int_{t_0}^{t_1}fdt + \int_{t_0}^{t_1}dt(\lambda g + \lambda' y) - \lambda y|_{t_0}^{t_1}
\end{equation}
Don't forget that here the arguments on the right hand side are $y$ and $u$. $u^*$ is the maximizing/optimal control so $J(a)$ is max at $a = 0 \to J'(0) = 0$:
\begin{equation}
J'(0) = \int_{t_0}^{t_1}\big[(\partial_x f + \lambda\partial_x g + \lambda')\partial_a y + (\partial_u f + \lambda\partial_u g)h\big]dt - \lambda(t_1)\partial_ay(t_1,0) = 0
\end{equation}
The derivatives are with respect to the arguments of the functions as they have been discussed so far...it should make sense. To make this equation work we need
\begin{align}
\lambda(t_1) = 0 \\
\lambda'(t) = -\big[\partial_xf(t,x^*,u^*) + \lambda(t)\partial_xg(t,x^*,u^*)\big] \\
\int_{t_0}^{t_1}(\partial_uf + \lambda\partial_ug)hdt = 0, \text{ for any h we can think of}
\end{align}
This last statement means that the object in the parethesis must be zero:
\begin{equation}
\partial_uf(t,x^*,u^*) + \lambda(t)\partial_ug(t,x^*,u^*) = 0
\end{equation}

Therefore, if the functions $u^*(t)$ and $x^*(t)$ maximize $\int_{t_0}^{t_1}fdt$ subject to $x' = g$, there exists a continuously differentiable function $\lambda(t)$ such that $u^*,x^*$ and $\lambda$ simultaneously satisfy the
\begin{align}
\text{state equation } x'(t) = g(t,x(t),u(t)), x(t_0)= x_0 \\
\text{multiplier equation } \lambda'(t) = -[f_x(t,x,u) + \lambda(t)g_x(t,x,u)], \lambda(t_1) = 0 \\
\text{and the optimality equation } f_u(t,x,u) + \lambda(t)g_u(t,x,u) = 0
\end{align}
The multiplier equation is often referred to as the costate equation. As with variational calculus, we are lead to define a function with a set of properties that encapsulates these equations, so that we don't have to compute the preceding derivation every time. So, let's define a Hamiltonian
\begin{equation}
H = f(t,x,u) + \lambda(t)g(t,x,u)
\end{equation}
so that the state, multiplier and optimality equations are
\begin{align}
\frac{\partial H}{\partial \lambda} = x' \\
\frac{\partial H}{\partial x} = -\lambda' \\
\text{and } \frac{\partial H}{\partial u} = 0
\end{align}
with the additional boundary conditions $x(t_0) = x_0$ and $\lambda(t_1) = 0$. Since $\partial_uH = 0$, $u$ is a stationary point of $H$ at all times and we can integrate that equation (the optimality equation) to find $u$ as a function of $x$ and $\lambda$ and substitute that into the state and multiplier equations to get a system of 2 differential equations in $x$ and $\lambda$. Thus, we have solved our problem. Also, check out how these equations look just like Hamiltons equations for generalized momenta and coordinates.

\subparagraph{Multivariable Generalization}
Now that we have solved what we can consider to be the simplest optimal control problem, we should start generalizing (at least as far as we need for the project). So this time start with 2 state and 2 control variables:
\begin{align}
\text{maximize } \int_{t_0}^{t_1}f(t,x_1,x_2,u_1,u_2)dt \\
\text{with } x'_i = g_i(t,x_1,x_2,u_1,u_2) \\
\text{and } x_1(t_0) = x_2(t_0) \text{ fixed}
\end{align}
By the exact same procedure as the 1 variable case, we have
\begin{equation}
\int_{t_0}^{t_1}fdt = \int_{t_0}^{t_1}(f + \lambda_1g_1 + \lambda_2g_2 + \lambda'_1y_1 + \lambda'_2y_2)dt - (\lambda_1y_1 + \lambda_2y_2)|_{t_0}^{t_1} = J(a)
\end{equation}
where $f$ and $g_i$ are functions of $y_1$ and $y_2$ now. Then the extrema equation is 
\begin{align}
J'(0) = \int_{t_0}^{t_1}\big[(\partial_{x_1}f + \lambda_1\partial_{x_1}g_1 + \lambda_2\partial_{x_1}g_2 + \lambda'_1)\frac{\partial y_1}{\partial a} \\
+ (\partial_{x_2}f + \lambda_1\partial_{x_2}g_1 + \lambda_2\partial_{x_2}g_2 + \lambda'_1)\frac{\partial y_2}{\partial a} \\
+ (\partial_{u_1}f + \lambda_1\partial_{u_1}g_1 + \lambda_2\partial_{u_1}g_2)h_1 + (\partial_{u_2}f + \lambda_1\partial_{u_2}g_1 + \lambda_2\partial_{u_2}g_2)h_2\big]dt \\
- \lambda_1(t_1)\partial_ay_1(t_1) - \lambda_2(t_1)\partial_ay_2(t_1) + \lambda_1(t_0)\partial_ay_1(t_0) + \lambda_2(t_0)\partial_ay_2(t_0) = 0
\end{align}
giving us the equations
\begin{align}
\lambda'_1 = -[\partial_{x_1}f + \lambda_1\partial_{x_1}g_1 + \lambda_2\partial_{x_1}g_2], \lambda_1(t_1) = 0 \\
\lambda'_2 = -[\partial_{x_2}f + \lambda_1\partial_{x_2}g_1 + \lambda_2\partial_{x_2}g_2], \lambda_2(t_1) = 0 \\
\partial_{u_1}f + \lambda_1\partial_{u_1}g_1 + \lambda_2\partial_{u_1}g_2 = 0 \\
\partial_{u_2}f + \lambda_1\partial_{u_2}g_1 + \lambda_2\partial_{u_2}g_2 = 0
\end{align}
Obviously the Hamiltonian is $H = f + \lambda_1g_1 + \lambda_2g_2$ so that we have the 
\begin{align}
\text{state equations } x'_i = \frac{\partial H}{\partial \lambda_i} = g_i \\
\text{multiplier equations } \lambda'_i = -\frac{\partial H}{\partial x_i} \\
\text{optimality equations } 0 = \frac{\partial H}{\partial u_i}
\end{align}
We should be able to see immediately how this extends to n state variables and m control variables:
\begin{equation}
H(t,\vec{x},\vec{u},\vec{\lambda}) = f(t,\vec{x},\vec{u}) + \sum_{i=1}^{n} \lambda_ig_i(t,\vec{x},\vec{u})
\end{equation}
giving us the general multivariable equations of
\begin{align}
\text{state } x'_i = \frac{\partial H}{\partial \lambda_i} = g_i(t,\vec{x},\vec{u}) \\
\text{multipliers } \lambda'_i = -\frac{\partial H}{\partial x_i} = -\big(\frac{\partial f}{\partial x_i} + \sum_{j=1}^{n} \lambda_j\frac{\partial g_j}{\partial x_i}\big) \\
\text{optimality } 0 = \frac{\partial H}{\partial u_i} = \frac{\partial f}{\partial u_i} + \sum_{j=1}^{n} \lambda_j\frac{\partial g_j}{\partial u_i}
\end{align}
Now we can solve problems with any arbitrary number of state variables - those that we're really trying to solve for - in terms of any number of control functions. But we're not quite done. We might need to change our boundary conditions, whether it be fixing both ends, fixing the terminal point, not fixing the ends or maybe even giving some inequality relation on them (for instance have $x(t_1) > x_0$). So we need to extend this formulation to incorporate those types of problems.

\subparagraph{Fixed Endpoint Problems}
Of these specific BC alternatives, let's consider initial and terminal state values to be fixed. The problem statement is now to maximize
\begin{align}
\int_{t_0}^{t_1}f(t,x,u)dt \\
\text{so that } x'(t) = g(t,x,u) \text{ with} \\
x(t_0) = x_0 \text{ and } x(t_1) = x_1 \text{ both fixed }
\end{align}
We'll just take one state variable and one control to simplify things, since now we know how to extend our results to any number. Our procedure is slightly different but we're using the same notation as before, so consider the difference in the functional in question from its maximum value to some arbitrary one
\begin{align}
\Delta J \equiv J - J^* = \int_{t_0}^{t_1}\big[f(t,x,u) + \lambda g(t,x,u) + \lambda'x \\
- f(t,x^*,u^*) - \lambda g(t,x^*,u^*) + \lambda'x^*\big]dt
\end{align}
Next Taylor expand around $(t,x^*,u^*)$ to get
\begin{align}
\Delta J = \int_{t_0}^{t_1}\big[(\partial_x f +\lambda\partial_x j + \lambda')(x - x^*) \\
+ (\partial_u f + \lambda \partial_u g)(u - u^*)\big]dt + \int_{t_0}^{t_1}...
\end{align}
Defining variations in the usual sense we have that
\begin{equation}
\delta J = \int_{t_0}^{t_1}\big[(f_x + \lambda g_x + \lambda')\delta x + (f_u \lambda g_u)\delta u\big]dt
\end{equation}
is the first order variation of J. Therefore in order to make $\delta J = 0$ we require that
\begin{align}
\lambda' = -(\partial_x f + \lambda\partial_x g) \\
0 = \partial_u f + \lambda\partial_u g
\end{align}
So if $x^*$ and $u^*$ are the optimal solutions, there exists a function $\lambda$ such that $x^*, u^*, \lambda$ satisfy these last 2 equations along with the 2 boundary conditions. Notice that in the second to last equation we don't impose a boundary condition on $\lambda$. This is because the constant of integration is found by one of the boundary conditions on x. The other condition is used for the constant from the integration of $x'$ directly.
This procedure is very familiar. The two boundary conditions are exactly what we get in most variational problems we consider in physics. Take the brachistochrone problem for example (my favorite). The situation is a functional giving the time of flight of a particle along the trajectory $x$, where we know exactly where it starts and where it ends. Minimizing this functional yields the $x$ that gets the particle from $x_0$ to $x_1$ in the shortest time. That's exactly the situation we've got here, and why we are all of a sudden considering variations the way we see them in variational calculus (the aforementioned Frechet derivative).

\subparagraph{Generalized Endpoint Conditions}
Now we're in a position to consider general endpoint requirements as we will encounter them in the project. This is the final topic of this discussion on optimal control. So sad. Bunching them all in 1 multivariable problem, we would like to maximize
\begin{align}
\int_{t_0}^{t_1}f(t,\vec{x},\vec{u})dt + \phi(t,\vec{x}(t_1)) \\
\text{subject to } x'_i = g_i, \\
x_i(t_0) = x_{i0}, \\
x_k(t_1) = x_{k1} \text{ for }k = 1...q, \\
x_j(t_1) \text{ free for }j = q+1...r, \\
x_l(t_1) \geq 0 \text{ for } l = r+1...s, \\
\text{and }K(x_{s+1}...x_n,t_1) \geq 0
\end{align}
What this means in words is that the first q state variables end at fixed points. The next r - q have free endpoints. The next s - r are strictly nonnegative, and the last n - s are ``entangled" in some smooth function K. We need to only consider the last 3 statements as new problems.

In the same fashion as before we have our functional
\begin{equation}
J = \int_{t_0}^{t_1}[f + \vec{\lambda}\cdot(\vec{g} - \vec{x}')]dt + \phi(t_1,\vec{x}(t_1))
\end{equation}
and upon integrating by parts to get rid of the $x'$
\begin{equation}
J = \int_{t_0}^{t_1}[f + \vec{\lambda}\cdot\vec{g} + \vec{\lambda}'\cdot\vec{x}]dt - \vec{\lambda}\cdot\vec{x}|_{t_0}^{t_1} + \phi(t_1,\vec{x}(t_1))
\end{equation}
You may be wondering what the fuck that $\phi$ is doing in there. Full disclosure: I'm not entirely certain but I have accepted that it's necessary for some of the boundary conditions. The text refers to it as a salvage term, so it's clearly meant to save something from being lost. Its derivatives show up in conditions when $x_1$ is free, greater than zero or identically zero. That's another confusing part to me which we'll get to in a bit. Note however that it is some \textit{number} that comes from what's going on at $t_1$.

Now, let's denote $J^*, f^*, g^*$ and $\phi^*$ as the values attained by passing along $(\vec{x}^*,\vec{u}^*)$, and let $(\vec{x},\vec{u})$ be nearby feasible paths that satisfy the boundary conditions...on the interval $[t_0,t_1+\delta t_1]$. I don't know why but I'm determined to figure it out. Something similar happens in Feynman's book on path integrals, and I didn't quite understand it there either though I spent a lot of time trying. We'll accept it for now and then move on. Computing the difference in $J$ along the two paths,
\begin{align}
J - J^* = \int_{t_0}^{t_1}(f + \vec{\lambda}\cdot\vec{g} + \vec{\lambda}'\cdot\vec{x} - f^* - \vec{\lambda}\cdot\vec{g}^* - \vec{\lambda}'\cdot\vec{x}^*)dt + \phi - \phi^* \\
- \vec{\lambda}(t_1)\cdot(\vec{x}(t_1) - \vec{x}^*(t_1)) + \int\limits_{t_0}^{t_1+\delta t_1}f(t,\vec{x},\vec{u})dt
\end{align}
Performing a series expansion of the nonoptimal terms about $(\vec{x}^*,\vec{u}^*)$ in $(\vec{x},\vec{u})$ and keeping only the first order terms we obviously get the familiar first order variation. Using the following abbreviations
\begin{align}
h_i(t) = x_i(t) - x_i^*(t) \\
\delta u_i(t) = u_i(t) - u_i^*(t) \\
\text{and } \delta\vec{x}_1 = \vec{x}(t_1 + \delta t_1) - \vec{x}^*(t_1)
\end{align}
\begin{align}
\delta J = \int_{t_0}^{t_1}\big[(\partial_x f + \vec{\lambda}\cdot\partial_x g + \vec{\lambda}')\cdot\vec{h} + (\partial_u f + \vec{\lambda}\cdot\partial_u g)\cdot\delta\vec{u}\big]dt \\
+{\color{red}\partial_x\phi\cdot\delta\vec{x}_1 + \partial_t\phi\delta t_1} - \vec{\lambda}(t_1)\cdot\vec{h}(t_1) + {\color{red}f(t_1)\delta t_1}
\end{align}
The terms highlighted in red need some explaining, although I don't really know how to...

Can now make an approximation that I don't fully understand but seems feasible:
\begin{equation}
\vec{h}(t_1) = \vec{x}(t_1) - \vec{x}^*(t_1) \approx \delta\vec{x}_1 - \vec{x}'^*(t_1)\delta t_1 = \delta\vec{x}_1 - \vec{g}^*(t_1)\delta t_1
\end{equation}
Inserting into the functional variation give us
\begin{align}
\delta J = \int_{t_0}^{t_1}\big[(f_x + \vec{\lambda}\vec{g}_x + \vec{\lambda}')\cdot\vec{h} + (f_u + \vec{\lambda}\vec{g}_u)\cdot\delta\vec{u}\big]dt \\
+ (\phi_x - \vec{\lambda}(t_1))\cdot\delta\vec{x}_1 + (f + \vec{\lambda}\cdot\vec{g} + \phi_t)|_{t_1}\delta t_1 = 0
\end{align}
Thus we require that
\begin{align}
\vec{\lambda}' = -(f_x + \vec{\lambda}\vec{g}_x) \\
f_u + \vec{\lambda}\vec{g}_u = 0 \\
\text{and } (\phi_x - \vec{\lambda}(t_1))\cdot\delta\vec{x}_1 + (f + \vec{\lambda}\cdot\vec{g} + \phi_t)|_{t_1}\delta t_1 = 0
\end{align}
We can see that the first two equations are the costate equation and the optimality equation. We also of course have the state equation with these, but the introduction of this salvage term $\phi$ at the beginning of the problem and letting the feasible solutions past $t_1$ - or just letting $t_1$ vary I guess - gives rise to this third equation.

Don't be alarmed by the lack or abundance of vector arrows on the terms. Two vectors next to each other of course indicate an inner product, but the subscript indicates a differential turning the vector into a Jacobian matrix. The dot is included for those operations that result in a real number only.

Now we start to make demands on those last three boundary conditions we haven't encountered before and things get a little dicey. When $\vec{x}(t_1)$ is free, $\delta\vec{x}_1$ may have any sign so we must have that
\begin{equation}
\phi_x = \vec{\lambda}(t_1)
\end{equation}
if we consider that last equation and take $\delta t_1$ to be zero. My guess is that there's no reason to take the feasible equations beyond $t_1$ for this condition, so that we can just let that second term, $(f + \vec{\lambda}\cdot\vec{g} + \phi_t)|_{t_1}\delta t_1$ be zero from $\delta t_1$.

Now what about the second to last boundary condition? Considering just $\vec{x}(t_1) = 0$, $\delta\vec{x}_1$ may have any sign so again we have $\phi_x = \vec{\lambda}(t_1)$. If on the other hand $\vec{x}(t_1) > 0$

\section{Relevant Numerical Algorithms}

\subsection{Golden Section Search}

\subsection{Aerodynamic Coefficient Modeling}

\subsection{Modeling Atmospheric Density}

\subsection{Modeling the Speed of Sound}



\section{Orbital Mechanics Concerned}





\begin{thebibliography}{99}

\bibitem{Nicolaescu}
  Nicolaescu, Liviu (2007). Lectures on the Geometry of Manifolds (2nd ed.). World Scientific. ISBN 978-9811214813. 
  
\bibitem{Goldstein}
  Goldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd ed.). Addison Wesley. ISBN 978-0-201-65702-9.
  
\bibitem{FetWal}
  Fetter, Alexander; Walecka, John Dirk (2003). Theoretical Mechanics of Particles and Continua. Dover Publications. ISBN 978-0486432618.

\bibitem{Main}
  Greg Dukeman. "Atmospheric Ascent Guidance for Rocket-Powered Launch Vehicles," AIAA 2002-4559. AIAA Guidance, Navigation, and Control Conference and Exhibit. August 2002. 
  
\bibitem{OC}
  Kamien, M. I.; Schwartz, N. L. (1991). Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management (Second ed.). New York: Elsevier. ISBN 0-444-01609-0.
  

\end{thebibliography}


\end{document}
